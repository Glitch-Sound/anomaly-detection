[MODEL]
; learning_rate: オプティマイザの初期学習率。大きいほど学習は速く進むが発散しやすく、小さいほど安定するが収束が遅くなる。
learning_rate = 0.001
; batch_size: 1 回の更新で処理する画像枚数。増やすと統計が安定するが GPU メモリを多く消費し、減らすと学習が不安定になりやすい。
batch_size = 4
; num_epochs: 学習データ全体を繰り返す回数。多いほど精度向上が期待できるが過学習や計算時間の増大につながる。
num_epochs = 10
; input_size: 入力画像をリサイズする一辺の長さ。大きくすると細部を捉えやすいが計算コストとメモリ使用量が増える。
input_size = 384
; flow_steps: 正規化フローの段数。段数を増やすと表現力が高まるが学習・推論が重くなる。
flow_steps = 16
; hidden_ratio: フローブロック内部のチャネル拡張率。値を大きくすると表現力が増すがパラメータ数とメモリが増える。
hidden_ratio = 1.2
; conv3x3_only: 真にすると 3x3 畳み込みのみを使用し、モデルを軽量化する代わりに受容野が狭くなる。
conv3x3_only = false
; backbone: 特徴抽出に利用する事前学習済みモデル名。モデルによって表現力・必要メモリ・推論速度が変わる。
backbone = deit_base_distilled_patch16_384
; vit_layers: ViT バックボーンから特徴を取得するブロック番号。値を変えると抽出される解像度や深さが変わり検出性能に影響する。
vit_layers = 9,12
; lr_scheduler_factor: 学習率を減衰させる際の乗数。0.5 なら学習率が半分になる。値が小さいほど大きく減衰する。
lr_scheduler_factor = 0.5
; lr_scheduler_patience: 指標が改善しないエポック数。この回数を超えると学習率が減衰する。大きいと減衰が遅くなる。
lr_scheduler_patience = 3
; lr_scheduler_min_lr: 学習率の下限値。これ以下には下がらず、極端な学習停止を防ぐ。
lr_scheduler_min_lr = 0.000001
; lr_scheduler_monitor: 学習率減衰の監視指標。train_loss や val_loss などを指定し、その改善が止まった場合に減衰が発動する。
lr_scheduler_monitor = train_loss


; timm: vit_large_patch16_512 (ViT-L/16; 512入力。性能とメモリ負荷のバランスが良く、事前学習強力)。
; timm: vit_huge_patch14_518 (518px対応。精度重視。A100/80GBクラス推奨)。
; timm: maxvit_large_tf_512 or maxvit_xlarge_512 (MaxViT系列。局所/グローバル両対応で高分解能向き)。
; timm: xcit_large_24_p8_512 (XCiT。512入力。ViT派生で比較的効率的)。
; timm: convnext_xlarge.fb_in22k_ft_in1k_640 (ConvNeXt XL; 640px。CNN系で安定、高分解能)。
; timm: eva_large_patch14_336.mim_in22k + 2×アップサンプルで 672/896 運用も可 (EVA系列は巨大モデル)。
; メモ

; 16パッチ系 → 入力は16の倍数 (512, 768, 992, 1008など) に丸める。
; 512超はVRAMが急増。バッチサイズ1やAMP必須なケース多い。
; バックボーン変更時は input_size・vit_layers をモデル仕様に合わせて調

; input_size と backbone の更新は必須ですが、それだけで完結しないケースがほとんどです。バックボーンを切り替えると、

; 取り出せる ViT 層が変わるため vit_layers の見直しが必要になることが多いです（そのモデルが持たない層番号を指定するとエラーになります）。
; 新しい入力サイズがパッチサイズの倍数か要確認。16パッチ系なら input_size を 16 の倍数に丸める必要があります。
; モデルによっては特徴量の次元が変わるため、後段のヘッド設定やしきい値にも影響が出ることがあります。
; まず input_size と backbone を調整し、そのバックボーンの仕様に合わせて vit_layers（必要ならその他の設定）を合わせる流れが安全です。



[THRESHOLD]
; epsilon: 閾値計算時の数値安定化パラメータ。小さくすると計算は鋭敏になるが安定性が下がる。
epsilon = 0.0001

[INFER]
; score_pooling: パッチスコアをサンプル単位に集約する方法。`max` は最大値を採用し、局所的な異常に敏感になる。
score_pooling = max
; score_q: スコア分布から閾値を決める際の分位点。値を上げると閾値が高くなり、異常判定が厳しくなる。
score_q = 0.999

[DATA]
; train_data_path: 学習に用いる正常画像ディレクトリ。ここに含まれるデータのみでモデルが正常パターンを学習する。
train_data_path = data/train/good/
; test_data_path: テストデータのルート。`good` と `error` サブフォルダから正常・異常サンプルを読み込む。
test_data_path = data/test/
; val_split_ratio: 学習データから検証用に確保する割合。大きくすると検証精度は安定するが学習に使える正常データが減る。
val_split_ratio = 0.1
; num_workers: DataLoader の並列読み込みプロセス数。増やすと I/O が速くなるが CPU リソースを消費する。
num_workers = 2

[OUTPUT]
; model_save_path: 学習済みモデルと閾値を保存するパス。再学習なしで再利用する場合はこのファイルを読み込む。
model_save_path = params/model.pth

[RESULT]
; output_dir: 推論結果・レポート・ヒートマップを出力するディレクトリ。削除すると再生成される。
output_dir = result

[HEATMAP]
; activation_percentile: ヒートマップ生成時に高スコア領域を抽出するパーセンタイル。値を下げると広い範囲が強調される。
activation_percentile = 0.7
; activation_min_value: ヒートマップの下限値。高くすると弱い異常は抑えられ、明確な異常だけを強調できる。
activation_min_value = 0.5
; blur_kernel_size: ヒートマップを平滑化するガウシアンブラーのカーネルサイズ。大きくすると滑らかさが増すが細部が失われる。
blur_kernel_size = 5
; blur_sigma: ガウシアンブラーの標準偏差。値を上げるほど平滑化が強くなり、ノイズが減る代わりに境界がぼやける。
blur_sigma = 1.0
; normalize_clip_percentile: 可視化時にスコアを正規化する上限パーセンタイル。小さくすると最大値に合わせた強いコントラストになる。
normalize_clip_percentile = 0.99
; normalize_clip_lower_percentile: スコア正規化の下限パーセンタイル。大きくすると弱い異常がカットされ、強調部分が限定される。
normalize_clip_lower_percentile = 0.0
; resize_to_input_size: 真にするとヒートマップを推論時の入力サイズに合わせてリサイズし、元画像に重ね合わせる際の精度を高める。
resize_to_input_size = true
